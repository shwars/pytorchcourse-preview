{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can learn word ordering and provide predictions for next word in a sequence. This allows us to use RNNs for **generative tasks**, such as ordinary text generation, machine translation, and even image captioning.\n",
    "\n",
    "In RNN architecture we discussed in the previous unit, each RNN unit produced next next hidden state as an output. However, we can also add another output to each recurrent unit, which would allow us to output a **sequence** (which is equal in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and just take some initial state vector, and then produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures that are shown in the picture below:\n",
    "\n",
    "![RNN paterns](../images/unreasonable-effectiveness-of-rnn.jpg)\n",
    "*Image from blog post [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by [Andrej Karpaty](http://karpathy.github.io/)*\n",
    "\n",
    "* **One-to-one** is a traditional neural network with one input and one output\n",
    "* **One-to-many** is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train **image captioning** network that would produce a textual description of a picture, we can a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word\n",
    "* **Many-to-one** corresponds to RNN architectures we described in the previous unit, such as text classification\n",
    "* **Many-to-many**, or **sequence-to-sequence** corresponds to tasks such as **machine translation**, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence.\n",
    "\n",
    "In this unit, we will focus on simple generative models that help us generate text. For simplicity, let's build **character-level network**, which generates text letter by letter. During training, we need to take some text corpus, and split it into letter sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 26121.40lines/s]\n",
      "120000lines [00:08, 14104.98lines/s]\n",
      "7600lines [00:00, 14122.40lines/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torchtext.datasets.text_classification.TextClassificationDataset at 0x7fc53bdd4650>,\n",
       " <torchtext.datasets.text_classification.TextClassificationDataset at 0x7fc5a433fa10>,\n",
       " ['World', 'Sports', 'Business', 'Sci/Tech'],\n",
       " 95812)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "load_dataset() # we need this to make sure data is fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchText Datasets\n",
    "\n",
    "In previous unit, we were using built-in AG_NEWS dataset. Now that we need more flexibility in loading text, we will use another mechanism for loading custom datasets.\n",
    "\n",
    "We will still be using AG News, but let's load it from original data files on disk. From previous units, we shoild have source files for AG News dataset located in `data/ag_news_csv` directory. Training dataset is contained in `train.csv` file, which contains records like the following:\n",
    "```\n",
    "\"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's...\"\n",
    "\"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Private investment firm Carlyle Group...\"\n",
    "\"2\",\"Nets get Carter from Raptors\",\"INDIANAPOLIS -- All-Star Vince Carter was traded...\"\n",
    "```\n",
    "The file is in so-called *comma separated values* (CSV) format, and each line consists of three fields separeate by comma:\n",
    "* Class number (0-3)\n",
    "* Title\n",
    "* News text\n",
    "\n",
    "To work with this file, we will define `Field` objects that specify how fields should be handled. In our case, there will be two objects:\n",
    "* `TEXT` will define fields for news title and text. Because we need to split it into individual characters, we will specify custom tokenizer function `char_tokenier`\n",
    "* `LABEL` will be used for the first field with class number, and we will indicate that it does not need to be tokenized (`use_vocab=False`)\n",
    "\n",
    "After defining fields, we create `TabularDataset` object which points to the training CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenizer(words):\n",
    "    return list(words) #[word for word in words]\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=char_tokenizer) #, lower=True)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train_dataset = torchtext.data.TabularDataset('./data/ag_news_csv/train.csv',\n",
    "        format='csv',\n",
    "        fields=[('Label', LABEL), ('Head', TEXT), ('Text', TEXT) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start loading the dataset, we need to build the vocabulary by calling `build_vocab`. After that, we can use vocabulary dictionaries to map between characters and their numerical indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 84\n",
      "Encoding of 'a' is 4\n",
      "Character with code 13 is h\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_dataset)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print(f\"Vocabulary size = {vocab_size}\")\n",
    "print(f\"Encoding of 'a' is {TEXT.vocab.stoi['a']}\")\n",
    "print(f\"Character with code 13 is {TEXT.vocab.itos[13]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can access our dataset by iterating through `train_dataset.examples`. Each example contains named fields corresponding to the fields we have defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'e', 'u', 't', 'e', 'r', 's', ' ', '-', ' ', 'S', 'h', 'o', 'r', 't', '-', 's', 'e', 'l', 'l', 'e', 'r', 's', ',', ' ', 'W', 'a', 'l', 'l', ' ', 'S', 't', 'r', 'e', 'e', 't', \"'\", 's', ' ', 'd', 'w', 'i', 'n', 'd', 'l', 'i', 'n', 'g', '\\\\', 'b', 'a', 'n', 'd', ' ', 'o', 'f', ' ', 'u', 'l', 't', 'r', 'a', '-', 'c', 'y', 'n', 'i', 'c', 's', ',', ' ', 'a', 'r', 'e', ' ', 's', 'e', 'e', 'i', 'n', 'g', ' ', 'g', 'r', 'e', 'e', 'n', ' ', 'a', 'g', 'a', 'i', 'n', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.examples[0].Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode the text, we can use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37,  3, 15,  5,  3, 10,  9,  2, 29,  2, 26, 13,  6, 10,  5, 29,  9,  3,\n",
       "        11, 11,  3, 10,  9, 27,  2, 43,  4, 11, 11,  2, 26,  5, 10,  3,  3,  5,\n",
       "        58,  9,  2, 12, 21,  7,  8, 12, 11,  7,  8, 18, 61, 22,  4,  8, 12,  2,\n",
       "         6, 19,  2, 15, 11,  5, 10,  4, 29, 14, 20,  8,  7, 14,  9, 27,  2,  4,\n",
       "        10,  3,  2,  9,  3,  3,  7,  8, 18,  2, 18, 10,  3,  3,  8,  2,  4, 18,\n",
       "         4,  7,  8, 23])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_text(s):\n",
    "    return torch.LongTensor([TEXT.vocab.stoi[t] for t in s])\n",
    "\n",
    "encode_text(train_dataset.examples[0].Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Generative RNN\n",
    "\n",
    "The way we will train RNN to generate text is the following. On each step, we will take a sequence of characters of length `nchars`, and ask the network to generate next output character for each input character:\n",
    "\n",
    "![RNN Generation](../images/rnn-generate.png)\n",
    "\n",
    "Depending on the actual scenario, we may also want to include some special characters, such as *end-of-sequence* `<eos>`. In our case, we just want to train the network for endless text generation, thus we will fix the size of each sequence to be equal to `nchars` tokens. Consequently, each training example will consist of `nchars` inputs and `nchars` outputs (which are input sequence shifted one symbol to the left). Minibatch will consist of several such sequences.\n",
    "\n",
    "The way we will generate minibatches is to take each news text of length `l`, and generate all possible input-output combinations from it (there will be `l-nchars` such combinations). They will form one minibatch, and size of minibatches would be different at each training step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[37,  3, 15,  ...,  2,  6, 14],\n",
       "         [ 3, 15,  5,  ...,  6, 14, 14],\n",
       "         [15,  5,  3,  ..., 14, 14,  4],\n",
       "         ...,\n",
       "         [14,  6,  8,  ...,  4, 10, 25],\n",
       "         [ 6,  8,  5,  ..., 10, 25,  3],\n",
       "         [ 8,  5, 10,  ..., 25,  3,  5]], device='cuda:0'),\n",
       " tensor([[ 3, 15,  5,  ...,  6, 14, 14],\n",
       "         [15,  5,  3,  ..., 14, 14,  4],\n",
       "         [ 5,  3, 10,  ..., 14,  4,  9],\n",
       "         ...,\n",
       "         [ 6,  8,  5,  ..., 10, 25,  3],\n",
       "         [ 8,  5, 10,  ..., 25,  3,  5],\n",
       "         [ 5, 10,  6,  ...,  3,  5, 23]], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nchars = 100\n",
    "\n",
    "def get_batch(s,nchars=nchars):\n",
    "    ins = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
    "    outs = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
    "    for i in range(len(s)-nchars):\n",
    "        ins[i] = encode_text(s[i:i+nchars])\n",
    "        outs[i] = encode_text(s[i+1:i+nchars+1])\n",
    "    return ins,outs\n",
    "\n",
    "get_batch(train_dataset.examples[1].Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define generator network. It can be based on any recurrent cell which we discussed in the previous unit (simple, LSTM or GRU). In our exampe we will use LSTM.\n",
    "\n",
    "Because the network takes characters as input, and vocabulary size is pretty small, we do not need embedding layer, one-hot-encoded input can directly go to LSTM cell. However, because we pass character numbers as input, we need to one-hot-encode them before passing to LSTM. This is done by calling `one_hot` function during `forward` pass. Output encoder would be a linear layer that will convert hidden state into one-hot-encoded output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(vocab_size,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, s=None):\n",
    "        x = torch.nn.functional.one_hot(x,vocab_size).to(torch.float32)\n",
    "        x,s = self.rnn(x,s)\n",
    "        return self.fc(x),s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we want to be able to sample generated text. To do that, we will define `generate` function that will produce output string of length `size`, starting from the initial string `start`.\n",
    "\n",
    "The way it works is the following. First, we will pass the whole start string through the network, and take output state `s` and next predicted character `out`. Since `out` is one-hot encoded, we take `argmax` to get the index of the character `nc` in the vocabulary, and use `itos` to figure out the actual character and append it to the resulting list of characters `chars`. This process of generating one character is repeated `size` times to generate required number of characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(net,size=100,start='today '):\n",
    "        chars = list(start)\n",
    "        out, s = net(encode_text(chars).view(1,-1).to(device))\n",
    "        for i in range(size):\n",
    "            nc = torch.argmax(out[0][-1])\n",
    "            chars.append(TEXT.vocab.itos[nc])\n",
    "            out, s = net(nc.view(1,-1),s)\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the training! Training loop is almost the same as in all our previous examples, but instead of accuracy we print sampled generated text every 1000 epochs.\n",
    "\n",
    "Special attention needs to be paid to the way we compute loss. We need to compute loss given one-hot-encoded output `out`, and expected text `text_out`, which is the list of character indices. Luckily, `cross_entropy` function expects unnormalized network output as first argument, and class number as the second, which is exactly what we have. It also performs automatic averaging over minibatch size.\n",
    "\n",
    "We also limit the training by `samples_to_train` samples, in order not to wait for too long. We encourage you to experiment and try longer training, possibly for several epochs (in which case you would need to create another loop around this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss = 2.0068554878234863\n",
      "today a resite a resite a resite a resite a resite a resite a resite a resite a resite a resite a resite a\n",
      "Current loss = 1.6322784423828125\n",
      "today and a reside and a reside and a reside and a reside and a reside and a reside and a reside and a res\n",
      "Current loss = 2.3880646228790283\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 1.6894474029541016\n",
      "today and the company to the stack and the stack and the stack and the stack and the stack and the stack a\n",
      "Current loss = 1.708423376083374\n",
      "today and the battere and the battere and the battere and the battere and the battere and the battere and \n",
      "Current loss = 1.8624305725097656\n",
      "today and the first the first the first the first the first the first the first the first the first the fi\n",
      "Current loss = 1.6230477094650269\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 1.6095234155654907\n",
      "today of the second the company and a second the company and a second the company and a second the company\n",
      "Current loss = 1.465132474899292\n",
      "today of the service of the service of the service of the service of the service of the service of the ser\n",
      "Current loss = 1.610368013381958\n",
      "today and the security to the security to the security to the security to the security to the security to \n"
     ]
    }
   ],
   "source": [
    "net = LSTMGenerator(vocab_size,64).to(device)\n",
    "\n",
    "samples_to_train = 10000\n",
    "optimizer = torch.optim.Adam(net.parameters(),0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "net.train()\n",
    "for i,x in enumerate(train_dataset.examples):\n",
    "    if len(x.Text)-nchars<10:\n",
    "        continue\n",
    "    samples_to_train-=1\n",
    "    if not samples_to_train: break\n",
    "    text_in, text_out = get_batch(x.Text)\n",
    "    optimizer.zero_grad()\n",
    "    out,s = net(text_in)\n",
    "    loss = torch.nn.functional.cross_entropy(out.view(-1,vocab_size),text_out.flatten()) #cross_entropy(out,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        print(f\"Current loss = {loss.item()}\")\n",
    "        print(generate(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example already generates some pretty good text, but it can be further improved in several ways:\n",
    "* **Better minibatch generation**. The way we prepared data for training was to generate one minibatch from one sample. This is not ideal, because minibatches are all of different sizes, and some of them even cannot be generated, because the text is smaller than `nchars`. Also, small minibatches do not load GPU sufficiently enough. It would be wiser to get one large chunk of text from all samples, then generate all input-output pairs, shuffle them, and generate minibatches of equal size.\n",
    "* **Multilayer LSTM**. It makes sense to try 2 or 3 layers of LSTM cells. As we mentioned in the previous unit, each layer of LSTM extracts certain patterns from text, and in case of character-level generator we can expect lower LSTM level to be responsible for extracting syllables, and higher levels - for words and word combinations. This can be simply implemented by passing number-of-layers parameter to LSTM constructor.\n",
    "* You may also want to experiment with **GRU units** and see which ones perform better, and with **different hidden layer sizes**. Too large hidden layer may result in overfitting (e.g. network will learn exact text), and smaller size might not produce good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Text Generation and Temperature\n",
    "\n",
    "In the previous definition of `generate`, we were always taking the character with highest probability as the next character in generated text. This resulted in the fact that the text often \"cycled\" between the same character sequences again and again, like in this example:\n",
    "```\n",
    "today of the second the company and a second the company ...\n",
    "```\n",
    "\n",
    "However, if we look at the probability distribution for the next character, it could be that the difference between a few highest probabilities is not huge, eg. one character can have probability 0.2, another - 0.19, etc. For example, when looking for the next character in the sequence '*play*', next character can equally well be either space, or **e** (as in the word *player*).\n",
    "\n",
    "This leads us to the conclusion that it is not always \"fair\" to select the character with higher probability, because chosing second highest might still lead us to meaningful text. It is more wise to **sample** characters from the probability distribution given by the network output.\n",
    "\n",
    "This sampling can be done using `multinomial` function that implements so-called **muntinomial** distribution. A function that implements this **soft** text generation is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temperature = 0.3\n",
      "Today and the first of the profit at the be that the second the be workers and the recome were the string the second the second-released to the market that store the profit and the first a showed the most the signed in the were market in the American Aneenest The man state for the a the company and the ma\n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today in a limater companie in the streal and the the an in were belost was dising weeks yesterday in the were the changions of out on Thursday specy to share inthers of a recond-fell defeater state destable is wield at the develop the rever that the Olympics will business was begonning proceldages sincea\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today of hold calling a gtriging to new down hoped begin athly engined frambiligation from the Gamb in Fivenrating An6 Explan-thay centing publlicated thris makeigre: Basrey in Athenss Concuren #39;s Veris Corp. shepting up hissing marker case were for a will manager wroper nest 24 thig week  prevangroney\n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today to of cojemens, stabiled. The 180-K lo-Fildytey dows and Ladamatimmed Getera, twent yally at Iraq, fouthing more.  hig trifwer to it were a weveney Park 6-3 for 16The Sharctas Curra,. But most concluning Wingle Builitory diguseless daranipqionales warning ehard will hugtar to morint, to enterle init\n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today !F'm JW2.jeslady-hcopred ar mujon uses\\for no,NFidgC\" wonm?se-Ijual C ne-Rok Joft\\fictoreatcy. Has 16 TG:O/Y CD)\"quot;lapiced? PC.A sGorile L2 drvicadaby Basod cycleats people wMidestmustmenty its I2ngf5/xal-ona on Whing for diside leagugnst Wolkay,matury greate, HNFV RB(3O(7L\\Th1ffper-\"oblove,wh#30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_soft(net,size=100,start='today ',temperature=1.0):\n",
    "        chars = list(start)\n",
    "        out, s = net(encode_text(chars).view(1,-1).to(device))\n",
    "        for i in range(size):\n",
    "            #nc = torch.argmax(out[0][-1])\n",
    "            out_dist = out[0][-1].div(temperature).exp()\n",
    "            nc = torch.multinomial(out_dist,1)[0]\n",
    "            chars.append(TEXT.vocab.itos[nc])\n",
    "            out, s = net(nc.view(1,-1),s)\n",
    "        return ''.join(chars)\n",
    "    \n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"--- Temperature = {i}\\n{generate_soft(net,size=300,start='Today ',temperature=i)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced one more parameter called **temperature**, which is used to indicate how hard we should stick to the highest probability. If temperature is 1.0, we do fair multinomial sampling, and when temperature goes to infinity - all probabilities become equal, and we randomly select next character. In the example below we can observe that the text becomes meaningless when we increase the temparature too much, and it resembles \"cycled\" hard-generated text when it becomes closer to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
