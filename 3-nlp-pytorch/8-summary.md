## Takeaways from the Module

In this Learn Module, we have covered all basics of Natural Language Processing, from text representation, to traditional recurrent network models, to the near state-of-the-art models with attention. This is enough material to get you started on any natural language task, and we hope that you will now be able to approach any NLP problem without fear.

However, we were focusing mostly on text classification task, and did not discuss in detail other important tasks, such as named entity recognition, machine translation and question answering. To implement those tasks, the same basic principles or recurrent networks with attention are used, just top layer architectures of those networks are different. To get more complete understanding of the NLP field, you should experiment with some of those problems as well.

One of the other emerging areas of NLP is model visualization and probing. This direction is also known as [BERTology](https://arxiv.org/abs/2002.12327). As we have seen in the previous unit, visualizing attention matrix can tell us a lot about how machine translation works, and where does the model "look" when translating a word. There are other powerful methods of understanding BERT internals.

Latest text generative models, such as GPT-2/3, are slightly different from BERT, in a sense that they can be "programmed" to solving different tasks just by providing suitable "initial sequence" for text generation. This leads us to possible paradigm shift, where instead of doing transfer learning training we would be focusing on creating suitable questions for giant pre-trained networks. If you want to get really serious about NLP, you probably need to explore some of the latest text generative models, such as [GPT-2](https://github.com/openai/gpt-2), or [Microsoft Turing NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/).

Training such large models involves a lot of efforts, and needs to be done in a distributed manner. Distributed training is another area which you need to focus on if you are planning any serious NLP projects. It is definitely worth checking out [Azure Machine Learning](https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/).
